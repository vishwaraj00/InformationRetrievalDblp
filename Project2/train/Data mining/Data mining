data mining
knowledge-discovery in database
KDD
weka
K nearest neighbors
Classification
Decision Trees
Clustering
association Rules
Frequent Items 
 predictive
 statistical patterns
 prediction
 Segmentation
 Backpropagation
 Linear regression
 Probability Distributions
 Apriori
 classifier
 PageRank
 Naive Bayes
AdaBoost
expectation-maximization
k-means
classification and regression trees
Maximum likelihood estimation
k-nearest neighbor classification
CART
dataset 
Segmentation
Association
Clustering Query 
Regression 
Neural 
Sequence Clustering 
Time Series Model 
Prediction
 knowledge 
 discovery
 Anomaly
 cleaning
 In the 21st century the human beings are used in the different technologies to adequate in the
society . Each and every day the human beings are using the vast data and these data are in the
different fields .It may be in the form of documents, may be graphical formats ,may be the video
,may be records (varying array ) .As the data are available in the different formats so that the
proper action to be taken. Not only to analyze these data but also take a good decision and
maintain the data .As and when the customer will required the data should be retrieved from the
database and make the better decision .This technique is actually we called as a data mining or
Knowledge Hub or simply KDD(Knowledge Discovery Process).The important reason that
attracted a great deal of attention in information technology the discovery of useful
information from large collections of data industry towards field of “Data mining” is due to perception of “we are data rich but information poor”. There is huge volume of data but we
hardly able to turn them in to useful information and knowledge for managerial decision making
in business. To generate information it requires massive collection of data. It may be different
formats like audio/video, numbers, text, figures, Hypertext formats . To take complete advantage
of data; the data retrieval is simply not enough, it requires a tool for automatic summarization of
data, extraction of the essence of information stored, and the discovery of patterns in raw data.
With the enormous amount of data stored in files, databases, and other repositories, it is
increasingly important, to develop powerful tool for analysis and interpretation of such data
and for the extraction of interesting knowledge that could help in decision-making. The only
answer to all above is ‘Data Mining’. Data mining is the extraction of hidden predictive
information from large databases; it is a powerful technology with great potential to help
organizations focus on the most important information in their data warehouses [1,2,3,4].
Data mining tools predict future trends and behaviors, helps organizations to make proactive
knowledge-driven decisions [2]. The automated, prospective analyses offered by data mining move
beyond the analyses of past events provided by prospective tools typical of decision support
systems. Data mining tools can answer the questions that traditionally were too time
consuming to resolve. They prepare databases for finding hidden patterns, finding predictive
information that experts may miss because it lies outside their expectations.
Data mining, popularly known as Knowledge Discovery in Databases (KDD), it is the
nontrivial extraction of implicit, previously unknown and potentially useful information from data
in databases [3, 5]. It is actually the process of finding the hidden information/pattern of the
repositories .[1,3,5].
This paper describes 7 sections .Section 1 is completely introduction where you will get huge
information about the data mining concept. Section 2 describes the data mining task which describes
that how the data will be store, how to retrieve, how to analyze the data. .Section 3 focuses the data
mining classification tasks .section 4 provides the data mining life cycles. Section 5 describes
visualization of the data model and it involves extracting the hidden information as we as we have
proposed the new way to define KDD Process. Section 6 describes shortly, some of the popular
data mining methods. The chapter 7 is the heart of the paper, we have reviewed applications and we
propose feature directions some of data mining applications. We have added the scope of the data
mining applications so that the researcher can pin pointed the following areas. Data Mining Task  Exploratory Data Analy Discovering Patterns and Rules:
Data Mining Life Cycle Business Understanding Data Understanding Data Preparation: Modeling  Evaluation Visualizing Data Mining Model
The predictive model makes prediction about unknown data values by using the known values.
Ex. Classification, Regression, Time series analysis, Prediction etc. The descriptive model
identifies the patterns or relationships in data and explores the properties of the data
examined. Ex. Clustering, Summarization, Association rule, Sequence discovery etc.
Many of the data mining applications are aimed to predict the future state of the data.
Prediction is the process of analyzing the current and past states of the attribute and prediction of
its future state. Classification is a technique of mapping the target data to the predefined groups or
classes, this is a supervise learning because the classes are predefined before the examination of
the target data. The regression involves the learning of function that map data item to real valued
prediction variable. In the time series analysis the value of an attribute is examined as it varies
over time. In time series analysis is used for many statistical techniques which will analyze
the time-series data such as auto regression methods etc.It is some times used in the two
type of modeling (1) ARIMA (II)Long-memory time-series modeling .
The term clustering means analyzes the different data objects without consulting a known
class levels. It is also referred to as unsupervised learning or segmentation. It is the partitioning or
segmentation of the data in to groups or clusters. The clusters are defined by studying the
behavior of the data by the domain experts. The term segmentation is used in very specific context;
it is a process of partitioning of database into disjoint grouping of similar tuples. Summarization is
the technique of presenting the summarize information from the data. The association rule finds
the association between the different attributes. Association rule mining is a two-step process:
Finding all frequent item sets, Generating strong association rules from the frequent item sets.
Sequence discovery is a process of finding the sequence patterns in data. This sequence can be used
to understand the trend.
Decision Trees and Rules
Nonlinear Regression and Classification Methods
 Example-based Methods
Probabilistic Graphical Dependency Models
Relational Learning Models
We found these are some famous data mining methods are broadly classified as: On-Line
Analytical Processing ,(OLAP), Classification, Clustering, Association Rule Mining, Temporal
Data Mining, Time Series Analysis, Spatial Mining, Web Mining etc. These methods use
different types of algorithms and data. The data source can be data warehouse, database, flat file or
text file. The algorithms may be Statistical Algorithms, Decision Tree based, Nearest
Neighbor, Neural Network based, Genetic Algorithms based, Ruled based, Support Vector
Machine etc. Generally the data mining algorithms are fully dependent of the two factors these
are which type of data sets are using  what type of requirements of the user
Basing upon the above two factors the data mining algorithms are used.A knowledge discovery
(KD) process involves preprocessing data, choosing a data-mining algorithm, and post processing
the mining results. The Intelligent Discovery Assistants [7] (IDA), helps users in applying valid
knowledge discovery processes. The IDA can provide users with three benefits
Knowledge discovery in databases, data mining, surveys.
The Data Mine ([45]) includes pointers to downloadable
papers, and two large data mining bibliographies. It attempts to
provide links to as much of the available data mining
information on the net as is possible.
The Knowledge Discovery Mine ([44]) has the KDD FAQ, a
comprehensive catalog of tools for discovery in data, as well as
back issues of the KDD-Nuggets mailing list. The Exclusive Ore internet site ([18]) contains a data mining
product features table that compares key features of
approximately 15 of the top products and has links directly from
the features table to each product. The site, which is still under
construction, will also have tutorials on various data mining
technologies and problems, illustrated with real examples that,
at the same time, show how various products work. Kurt Thearling maintains a set of knowledge discovery
related WWW pages ([56]). Among others, there is a list more
than 60 data mining software vendors, a list with software
patents related to data mining, and general information (tutorials
and papers) related to data mining. There is still some confusion about the terms Knowledge
Discovery in Databases (KDD) and data mining. Often these two
terms are used interchangeably. We use the term KDD to denote
the overall process of turning low-level data into high-level
knowledge. A simple definition of KDD is as follows: Knowledge
discovery in databases is the nontrivial process of identifying
valid, novel, potentially useful, and ultimately understandable
patterns in data ([20]). We also adopt the commonly used
definition of data mining as the extraction of patterns or models
from observed data. Although at the core of the knowledge
discovery process, this step usually takes only a small part
(estimated at 15% to 25 %) of the overall effort ([8]). Hence data
mining is just one step in the overall KDD process. Other steps
for example involve:
Developing an understanding of the application domain and
the goals of the data mining process
 Acquiring or selecting a target data set
 Integrating and checking the data set
 Data cleaning, preprocessing, and transformation
 Model development and hypothesis building
 Choosing suitable data mining algorithms
 Result interpretation and visualization
 Result testing and verification
 Using and maintaining the discovered knowledge
 Any realistic knowledge discovery process is not linear, but rather
iterative and interactive. Any one step may result in changes in
earlier steps, thus producing a variety of feedback loops. This
motivates the development of tools that support the entire KDD
process, rather than just the core data-mining step. Such tools
require a tight integration with database systems or data
warehouses for data selection, preprocessing, integrating,
transformation etc.
Many tools currently available are generic tools from the AI or
statistics' community. Such tools usually operate separately from
the data source, requiring a significant amount of time spent with
data export and import, pre- and post-processing, and data
transformation. However, a tight connection between the
knowledge discovery tool and the analyzed database, utilizing the
existing DBMS support, is clearly desirable.
For the reviewed knowledge discovery tools, the following
features are inspected:
Ability to access a variety of data sources: In many cases, the data
to be analyzed is scattered throughout the corporation, it has to be
gathered, checked, and integrated before a meaningful analysis
can take place. The capability to directly access different data
sources can thus greatly reduce the amount of data transforming.
Online/Offline data access: Online data access means that queries
are run directly against the database and may run concurrently
with other transactions. In offline data access the analysis is
performed with a snapshot of the data source, in many cases
involving an export/import process from the original data source
to a data format required by the discovery tools. The question of
online or offline data access becomes especially important when
one has to deal with changing knowledge and data: In financial
markets for example, rapidly changing market conditions may
make previously discovered rules and patterns invalid.
The underlying data model: Many tools that are available today
just take their input in form of one table, where each sample case
(record) has a fixed number of attributes. Other tools are based on
the relational model and allow querying of the underlying
database. Object-oriented and nonstandard data models, such as
multimedia, spatial or temporal, are largely beyond the scope of
current KDD technology ([21]).
Maximum number of tables/rows/attributes: These are theoretical
limits on the processing capabilities of the discovery tool.
Database size the tool can comfortably handle: The anticipated
amount of data to be analyzed should be an important factor in
choosing a discovery tool. While the maximum numbers of
tables/rows/attributes are theoretical limitations, there are also
practical limitations that are posed by computing time, memory
requirements, expressing and visualization capabilities etc. A tool
that holds all data in main memory for example may be not
appropriate for very large data sources, even if the theoretical
maximum number of rows is unlimited.
Attribute types the tool can handle: Some discovery tools have
restrictions upon the attribute types of the input data. For
example, tools based on neural networks usually require all
attributes to be of numeric type ([33]). Other approaches such as
[49] may not be able to handle continuous (real) data, etc.
Therefore, the attribute types present in the data source should be
considered when selecting an analysis tool.
Query language: The query language acts as an interface between
the user and the knowledge and database. It allows the user to
process data and knowledge and to direct the discovery process.
Some tools do not have a query language: human interaction is
restricted to the specification of some process parameters. Others
allow querying of the data and/or knowledge via queries
formulated in some query language, which may be a standard
language like SQL or an application specific language. Querying
of data and knowledge may also take place via a graphical user
interface (GUI).
The reviewed tools differ greatly in each of the features described
above. The choice of a tool therefore depends on application
specific requirements and considerations, such as form and size of
the data available, goals of the discovery process, needs and
training of the end user, etc.
At the core of the KDD process are the data mining methods for
extracting patterns from data. These methods can have different
goals, dependent on the intended outcome of the overall KDD
process. It should also be noted that several methods with
different goals may be applied successively to achieve a desired
result. For example, to determine which customers are likely to
buy a new product, a business analyst might need to first use
clustering to segment the customer database, then apply
regression to predict buying behavior for each cluster.
Most data mining goals fall under the following categories:
Data Processing: Depending on the goals and requirements of the
KDD process, analysts may select, filter, aggregate, sample, clean
and/or transform data. Automating some of the most typical data
processing tasks and integrating them seamlessly into the overall
process may eliminate or at least greatly reduce the need for
programming specialized routines and for data export/import, thus
improving the analyst’s productivity.
Prediction: Given a data item and a predictive model, predict the
value for a specific attribute of the data item. For example, given a
predictive model of credit card transactions, predict the likelihood
that a specific transaction is fraudulent. Prediction may also be
used to validate a discovered hypothesis.
Regression: Given a set of data items, regression is the analysis of
the dependency of some attribute values upon the values of other
attributes in the same item, and the automatic production of a
model that can predict these attribute values for new records. For
example, given a data set of credit card transactions, build a
model that can predict the likelihood of fraudulence for new
transactions.
Classification: Given a set of predefined categorical classes,
determine to which of these classes a specific data item belongs.
For example, given classes of patients that correspond to medical
treatment responses, identify the form of treatment to which a new
patient is most likely to respond.
Clustering: Given a set of data items, partition this set into a set of
classes such that items with similar characteristics are grouped
together. Clustering is best used for finding groups of items that
are similar. For example, given a data set of customers, identify
subgroups of customers that have a similar buying behavior.
Link Analysis (Associations): Given a set of data items, identify
relationships between attributes and items such as the presence of
one pattern implies the presence of another pattern. These
relations may be associations between attributes within the same
data item (‘Out of the shoppers who bought milk, 64% also
purchased bread’) or associations between different data items
(‘Every time a certain stock drops 5%, a certain other stock raises
13% between 2 and 6 weeks later’). The investigation of
relationships between items over a period of time is also often
referred to as ‘sequential pattern analysis’.
Model Visualization: Visualization plays an important role in
making the discovered knowledge understandable and
interpretable by humans. Besides, the human eye-brain system
itself still remains the best pattern-recognition device known.
Visualization techniques may range from simple scatter plots and
histogram plots over parallel coordinates to 3D movies.
Exploratory Data Analysis (EDA): Exploratory data analysis
(EDA) is the interactive exploration of a data set without heavy
dependence on preconceived assumptions and models, thus
attempting to identify interesting patterns. Graphic representations
of the data are used very often to exploit the power of the eye and
human intuition. While there are dozens of software packets available that were developed exclusively to support data
exploration, it might also be desirable to integrate these
approaches into an overall KDD environment.
It should be clear from the above that data mining is not a single
technique, any method that will help to get more information out
of data is useful. Different methods serve different purposes, each
method offering its own advantages and disadvantages. However,
most methods commonly used for data mining can be classified
into the following groups.
Statistical Methods: Historically, statistical work has focused
mainly on testing of preconceived hypotheses and on fitting
models to data. Statistical approaches usually rely on an explicit
underlying probability model. In addition, it is generally assumed
that these methods will be used by statisticians, and hence human
intervention is required for the generation of candidate hypotheses
and models.
Case-Based Reasoning: Case-based reasoning (CBR) is a
technology that tries to solve a given problem by making direct
use of past experiences and solutions. A case is usually a specific
problem that has been previously encountered and solved. Given a
particular new problem, case-based reasoning examines the set of
stored cases and finds similar ones. If similar cases exist, their
solution is applied to the new problem, and the problem is added
to the case base for future reference.
Neural Networks: Neural networks (NN) are a class of systems
modeled after the human brain. As the human brain consists of
millions of neurons that are interconnected by synapses, neural
networks are formed from large numbers of simulated neurons,
connected to each other in a manner similar to brain neurons. Like
in the human brain, the strength of neuron interconnections may
change (or be changed by the learning algorithm) in response to a
presented stimulus or an obtained output, which enables the
network to “learn”.
Decision Trees: A decision tree is a tree where each non-terminal
node represents a test or decision on the considered data item.
Depending on the outcome of the test, one chooses a certain
branch. To classify a particular data item, we start at the root node
and follow the assertions down until we reach a terminal node (or
leaf). When a terminal node is reached, a decision is made.
Decision trees can also be interpreted as a special form of a rule
set, characterized by their hierarchical organization of rules.
Rule Induction: Rules state a statistical correlation between the
occurrence of certain attributes in a data item, or between certain
data items in a data set. The general form of an association rule is
Xl ^ ... ^ Xn => Y [C, S], meaning that the attributes Xl,...,Xn
predict Y with a confidence C and a significance S.
Bayesian Belief Networks: Bayesian belief networks (BBN) are
graphical representations of probability distributions, derived
from co-occurrence counts in the set of data items. Specifically, a
BBN is a directed, acyclic graph, where the nodes represent
attribute variables and the edges represent probabilistic
dependencies between the attribute variables. Associated with
each node are conditional probability distributions that describe
the relationships between the node and its parents.
Genetic algorithms / Evolutionary Programming: Genetic
algorithms and evolutionary programming are algorithmic
optimization strategies that are inspired by the principles observed
in natural evolution. Of a collection of potential problem solutions
that compete with each other, the best solutions are selected and
combined with each other. In doing so, one expects that the
overall goodness of the solution set will become better and better,
similar to the process of evolution of a population of organisms.
Genetic algorithms and evolutionary programming are used in
data mining to formulate hypotheses about dependencies between
variables, in the form of association rules or some other internal
formalism.
Fuzzy Sets: Fuzzy sets form a key methodology for representing
and processing uncertainty. Uncertainty arises in many forms in
today’s databases: imprecision, non-specificity, inconsistency,
vagueness, etc. Fuzzy sets exploit uncertainty in an attempt to
make system complexity manageable. As such, fuzzy sets
constitute a powerful approach to deal not only with incomplete,
noisy or imprecise data, but may also be helpful in developing
uncertain models of the data that provide smarter and smoother
performance than traditional systems. Since fuzzy systems can
tolerate uncertainty and can even utilize language-like vagueness
to smooth data lags, they may offer robust, noise tolerant models
or predictions in situations where precise input is unavailable or
too expensive.
Rough Sets: A rough set is defined by a lower and upper bound of
a set. Every member of the lower bound is a certain member of the
set. Every non-member of the upper bound is a certain nonmember
of the set. The upper bound of a rough set is the union
between the lower bound and the so-called boundary region. A
member of the boundary region is possibly (but not certainly) a
member of the set. Therefore, rough sets may be viewed as fuzzy
sets with a three-valued membership function (yes, no, perhaps).
Like fuzzy sets, rough sets are a mathematical concept dealing
with uncertainty in data ([42]). Also like fuzzy sets, rough sets are
seldom used as a stand-alone solution; they are usually combined
with other methods such as rule induction, classification, or
clustering methods.
A discussion of the advantages and disadvantages for the different
approaches with respect to data mining as well as pointers to
introductory readings are given in [24].In this section we first provide a feature classification scheme to
study knowledge discovery and data mining tools. We then apply
this scheme to review existing tools that are currently available,
either as a research prototype or as a commercial product.
Although not exhaustive, we believe that the reviewed products
are representative for the current status of technology.
As discussed in Section 2, knowledge discovery and data mining
tools require a tight integration with database systems or data
warehouses for data selection, preprocessing, integrating,
transformation, etc. Not all tools have the same database
characteristics in terms of data model, database size, queries
supported, etc. Different tools may perform different data mining
tasks and employ different methods to achieve their goals. Some
may require or support more interaction with the user than the
other. Some may work on a stand-alone architecture while the
other may work on a client/server architecture. To capture all
these differences, we propose a feature classification scheme that
can be used to study knowledge discovery and data mining tools.
In this scheme, the tools’ features are classified into three groups
called general characteristics, database connectivity, and data
mining characteristics which are described below.
3.1.1 General Characteristics (Table 3.1)
Product: Name and vendor of the software product.
Production Status: Status of product development. P=Commercial
strength product, A=Alpha, B=Beta, R=Research Prototype.
Legal Status: PD=Public Domain, F=Freeware, S=Shareware,
C=Commercial.
Acad. Licensing: Specifies for commercial products, if there is
a special free or reduced-cost academic licensing available.
Demo: Specifies if there is a demo version available. D=Demo
version available for download on the internet, R=Demo available
on request, U=Unknown.
Architecture: The computer architecture on which the software
runs. S=Standalone, C/S=Client/Server, P=Parallel Processing.
Operating Systems: Lists the operating systems for which run time
version of the software can be obtained.
3.1.2 Database Connectivity (Table 3.2)
Data sources: Specifies possible formats for the data that is to be
analyzed. T=Ascii text files, D=Dbase files, P=Paradox files,
F=Foxpro files, Ix=Informix, O=Oracle, Sy=Sybase, Ig=Ingres,
A=MS Access, OC=Open database connection (ODBC), SS=MS
SQL Server, Ex=MS Excel, L=Lotus 1-2-3.
DB Conn.: Type of database connection. Onl.=Online: Queries
are run directly against the database and may run concurrently
with other transactions. Offl.=Offline: The analysis is performed
with a snapshot of the data source.
Size: The maximum number of records the software can
comfortably handle. S=Small (up to 10.000 records), M=Medium
(10.000 to 1.000.000 records), L=Large (more than 1.000.000
records).
Model: The data model for the data to be analyzed. R=Relational,
O = Object Oriented, 1=One table.
Attributes: The type of the attributes the software can handle.
Co=Continuous, Ca=Categorical (discrete numerical values),
S=Symbolic.
Queries: Specifies how the user can formulate queries against the
knowledge base and direct the discovery process. S=Structured
query language (SQL or derivative), Sp.=an application specific
interface language, G=Graphical user interface, N=Not applicable,
U=Unknown.
3.1.3 Data Mining Characteristics (Table 3.3)
Discovery Tasks: Knowledge discovery tasks that the product is
intended for. Pre.=Data Preprocessing (Sampling, Filtering),
P=Prediction, Regr=Regression, Cla=Classification,
Clu=Clustering, A=Link Analysis (Associations), Vis=Model
Visualization, EDA = Exploratory Data Analysis.
Discovery Methodology: Type of methods used to discover the
knowledge. NN=Neural Networks, GA=Genetic Algorithms, FS=Fuzzy Sets, RS=Rough Sets, St.=Statistical Methods,
DT=Decision Trees, RI=Rule Induction, BN=Bayesian Networks,
CBR = Case Based Reasoning.
Human Interaction: Specifies how much human interaction with
the discovery process is required and/or supported.
A=Autonomous, G=Human guided discovery process, H=Highly
Interactive.
In tables 3.1, 3.2, and 3.3, we apply our proposed feature
classification scheme to study 43 existing knowledge discovery
and data mining tools. Although the importance of the ability to
access a wide variety of data sources is widely recognized by now,
table 3.2 shows that the majority of currently available tools still
support only a small number of data formats. Surprisingly few
tools offer the ability to analyze several tables simultaneously.
However, almost all of the reviewed products can analyze
continuous as well as discrete and symbolic attribute types.
Mostly this is implemented by conversion of attribute types; i.e.
transforming symbolic variables into numerical ones or splitting
of continuous attribute ranges into intervals.
From table 3.3 we can observe that most of the tools employ
“standard” data mining techniques like rule induction, decision
trees, and statistical methods. Techniques from other promising
fields like fuzzy and rough sets or genetic algorithms have only
begun yet to find their way into knowledge discovery software.
Detailed descriptions for each software product are provided in
[24].Knowledge discovery can be broadly defined as the automated
discovery of novel and useful information from commercial
databases. Data mining is one step at the core of the knowledge
discovery process, dealing with the extraction of patterns and
relationships from large amounts of data.
Today, most enterprises are actively collecting and storing large
databases. Many of them have recognized the potential value of
these data as an information source for making business decisions.
The dramatically increasing demand for better decision support is
answered by an extending availability of knowledge discovery and
data mining products, in the form of research prototypes
developed at various universities as well as software products
from commercial vendors. In this paper, we provide an overview
of common knowledge discovery tasks, approaches to solve these
tasks, and available software tools employing these approaches.
However, despite its rapid growth, KDD is still an emerging field.
The development of successful data mining applications still
remains a tedious process ([21]). The following is a (naturally
incomplete) list of issues that are unexplored or at least not
satisfactorily solved yet:
Integration of different techniques. Currently available tools
deploy either a single technique or a limited set of techniques to
carry out data analysis. From section 2 it follows immediately that
there is no best technique for data analysis. The issue is therefore
not which technique is better than another, but rather which
technique is suitable for the problem at hand. A truly useful tool
(continued after results tables…)